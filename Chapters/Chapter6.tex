% Chapter 6

\chapter{Discussions} % Main chapter title

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter6}

\lhead{Chapter 6. \emph{Discussions}} % This is for the header on each page - perhaps a shortened title
The experiments results are interesting and kind of beyond our assumption. 
Firstly, in our assumption, the the MSE loss should be negatively correlated with the number of hidden layer units, but in fact, the MSE loss may oscillated a little and have the tendency but not much. Especially when lambda $\lambda$ become large, it's like hidden layer units number losses its influence on the MSE error. 

Secondly, the result about domain BCE loss seems promising, it's the same for training and for valid. When the lambda $\lambda$ is 0.1, we can see there's little influence about the neural network so the BCE loss is small, means we can still distinguish the session 1 and session 2. but when lambda $\lambda$ become large, the domain BCE loss stay large so that means we have reach our goal (make the representation of the 2 different session become indiscriminate).

Considering the above two parts, for this special subject who provided the EEG data, we should choose a lambda $\lambda$ larger than 10, and the number of hidden layer units should be [10,20] to get the balance between the performance and computation time.